{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f936d5b-fdc0-454c-a640-94e22ff9c1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.layers import LSTM,Dense,Dropout, Reshape, Conv1D, GlobalMaxPooling1D, MaxPooling1D\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,SimpleRNN\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import gensim\n",
    "\n",
    "import ast\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72c73832-cd71-4e72-98ab-7d8065513bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import os\n",
    "import io\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket('nlp_final_data')\n",
    "\n",
    "blob = bucket.blob('data_no_stop.csv')\n",
    "content = blob.download_as_string()\n",
    "\n",
    "df = pd.read_csv(io.BytesIO(content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bac0c75-127c-414c-95b3-443e5d5475c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data is saved a string, convert it back to a list of strings\n",
    "df['tokens_no_stop'] = df['tokens_no_stop'].apply(lambda x: x.strip('[]').replace('\\'', '').split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9b000376-2f57-481d-ba52-8306f7a1e017",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/jupyter/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import words\n",
    "\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "nltk.download('words')\n",
    "english_dictionary = set(words.words())\n",
    "\n",
    "def clean_tokens(words):\n",
    "#     # Remove single-character tokens (mostly punctuation)\n",
    "#     words = [word for word in words if len(word) > 1]\n",
    "\n",
    "#     # Remove numbers\n",
    "#     words = [word for word in words if not word.isnumeric()]\n",
    "\n",
    "#     # Remove punctuation\n",
    "#     words = [word for word in words if word.isalpha()]\n",
    "\n",
    "    # Lowercase all words (default_stopwords are lowercase too)\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    words = [word for word in words if word.lower() in english_dictionary]\n",
    "    \n",
    "    words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "83a3d2e7-86eb-42ea-a07d-d139ac82fddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['1-grams'] = df['tokens_no_stop'].apply(lambda x: clean_tokens(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd2f6d0e-6e35-43ca-9127-fb1d30e471ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['1-grams'] = df['1-grams'].apply(lambda x: x.strip('[]').replace('\\'', '').split(', '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "92508128-cad6-4baf-be23-bb951095a723",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2,3):\n",
    "    col_name = str(i)+'-grams'\n",
    "    df[col_name] = df['1-grams'].apply(lambda x: list(nltk.ngrams(x,i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b80087ba-b05d-4f78-9892-140090d90502",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['tag', '1-grams','2-grams', '3-grams']].to_csv('songs_nGrams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f690cbc-6c51-4c97-8b57-7dcfc724c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "import os\n",
    "import io\n",
    "client = storage.Client()\n",
    "bucket = client.get_bucket('nlp_final_data')\n",
    "\n",
    "blob = bucket.blob('songs_nGrams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af34c02b-1f3e-4989-a2eb-31826b4d5570",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'songs_nGrams.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_2509/355170923.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mblob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbucket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'songs_nGrams.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mblob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload_from_filename\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'songs_nGrams.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/google/cloud/storage/blob.py\u001b[0m in \u001b[0;36mupload_from_filename\u001b[0;34m(self, filename, content_type, num_retries, client, predefined_acl, if_generation_match, if_generation_not_match, if_metageneration_match, if_metageneration_not_match, timeout, checksum, retry)\u001b[0m\n\u001b[1;32m   2680\u001b[0m         \u001b[0mcontent_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_content_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2682\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfile_obj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2683\u001b[0m             \u001b[0mtotal_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfileno\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mst_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2684\u001b[0m             self.upload_from_file(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'songs_nGrams.csv'"
     ]
    }
   ],
   "source": [
    "blob = bucket.blob('songs_nGrams.csv')\n",
    "blob.upload_from_filename('songs_nGrams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "06e1390d-1148-44ba-889e-67232bcfd1ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('songs_nGrams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "afe505c0-0c91-47c1-bed0-da37e88a958e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tuple_to_list(tuple_list):\n",
    "    strings = []\n",
    "    for l in tuple_list:\n",
    "        ngrams = []\n",
    "        for ngram in l:\n",
    "            ngram_string = ' '.join(str(element) for element in ngram)\n",
    "            ngrams.append(ngram_string)\n",
    "            print(ngrams)\n",
    "        strings.append(ngrams)\n",
    "    return strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb7a6cfc-f0c8-4f0e-8372-4f27265f7103",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test = tuple_to_list(df['2-grams'][:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3fa1bb1f-919b-4fe2-a804-205f20db705b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df['3-grams_string'] = df['3-grams'].apply(lambda x: [' '.join(str(element) for element in tuple_) for tuple_ in x])\n",
    "df['2-grams_string'] = df['2-grams'].apply(lambda x: [' '.join(str(element) for element in tuple_) for tuple_ in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c44dbe08-d0f9-40b2-b7bb-ad5fc5d156f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "grams1 = df['1-grams']\n",
    "grams2 = df['2-grams_string']\n",
    "grams3 = df['3-grams_string']\n",
    "\n",
    "combined_grams = []\n",
    "for i in range(len(grams1)):\n",
    "    combined_grams.append(grams1[i]+grams2[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3529dad6-b7d2-4e0f-8616-7f1d7f701857",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['up_to_bigrams'] = combined_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e3ff5f-d554-4dcc-98ab-d75a280cf816",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_grams = []\n",
    "for i in range(len(grams1)):\n",
    "    combined_grams.append(grams1[i]+grams2[i]+grams3[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4846b10-32e3-403e-b1bf-343cb964095f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['up_to_trigrams'] = combined_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "6654b9df-030f-4729-89b2-295f022efcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['tag','up_to_trigrams', 'up_to_bigrams']].to_csv('songs_combined_ngrams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "58e95d01-b7d7-4097-a187-67cfb62a8047",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob = bucket.blob('songs_combined_ngrams.csv')\n",
    "blob.upload_from_filename('songs_combined_ngrams.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4e8f0fcc-6fba-41fc-987c-4ff9ffc00e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = list(df['1-grams'])\n",
    "y = df['tag']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a10692a-4b32-468a-b627-b13a9ced0167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://spotintelligence.com/2023/02/15/word2vec-for-text-classification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2b9b8ebb-1d70-4d17-be3b-9b9f9eb2433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the balanced data into training and testing sets\n",
    "#X_train, X_test, y_train, y_test = train_test_split(X_vec, y, test_size=0.2,stratify=y, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "69d3daa2-02bc-4ac9-b8ef-b6fe434e431c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = Tokenizer(char_level=False)\n",
    "tokenize.fit_on_texts(X)\n",
    "\n",
    "# we will need this later\n",
    "num_words = len(tokenize.word_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "959d50bb-62db-4876-8830-4ba060f017f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46207"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#num_words is 496351 with just tokens, and not cleaned\n",
    "#num_words is 46207 when data is lemmatized and cleaned\n",
    "num_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2996ea3b-28d8-4893-87f2-1b6af855857f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d649a20b-1bc0-4746-9dd8-d3e77f6abea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenize.texts_to_sequences(X_train)\n",
    "x_test = tokenize.texts_to_sequences(X_test)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y_train)\n",
    "y_train = encoder.transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "y_test = np_utils.to_categorical(y_test, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b7f206a9-a9f4-42e9-be35-838d39165f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_log_length = 1024\n",
    "x_train = pad_sequences(x_train, maxlen=max_log_length)\n",
    "x_test = pad_sequences(x_test, maxlen=max_log_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "97ad194e-5d1b-44ac-b7a2-58121735ed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Embedding(input_dim=num_words,output_dim=64,input_length=max_log_length))\n",
    "\n",
    "num_filters = 128\n",
    "kernel_sizes = [2,3]\n",
    "for kernel_size in kernel_sizes:\n",
    "    model3.add(Conv1D(num_filters, kernel_size, activation='relu'))\n",
    "#model3.add(Conv1D(128, 3, activation='relu'))\n",
    "\n",
    "model3.add(MaxPooling1D(pool_size=2))\n",
    "model3.add(LSTM(units=64,recurrent_dropout=0.5))\n",
    "model3.add(Dropout(0.5))\n",
    "\n",
    "# Add Dense layers\n",
    "model3.add(Dense(128, activation='relu'))\n",
    "model3.add(Dropout(0.5))\n",
    "    \n",
    "model3.add(Dense(num_classes,activation='softmax'))\n",
    "\n",
    "# compile the model\n",
    "model3.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01ae2b9-28f3-4605-89ee-b142aebeff00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a1d51651-ae6c-4464-946c-b5cd0f504e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "2438/2438 [==============================] - 1640s 672ms/step - loss: 1.1686 - accuracy: 0.5217 - val_loss: 1.0548 - val_accuracy: 0.5761\n",
      "Epoch 2/3\n",
      "2438/2438 [==============================] - 1640s 673ms/step - loss: 1.0396 - accuracy: 0.5836 - val_loss: 1.0421 - val_accuracy: 0.5757\n",
      "Epoch 3/3\n",
      "1284/2438 [==============>...............] - ETA: 12:40 - loss: 0.9887 - accuracy: 0.6041"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The Jupyter server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--ServerApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "ServerApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "ServerApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2438/2438 [==============================] - 1647s 676ms/step - loss: 0.9902 - accuracy: 0.6043 - val_loss: 1.0358 - val_accuracy: 0.5814\n"
     ]
    }
   ],
   "source": [
    "history = model3.fit(x_train, y_train,\n",
    "                    batch_size=128,\n",
    "                    epochs=3,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "3595fab9-2865-4255-8c34-f47f32980dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.save('model_0519.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "6fcd90ee-a5b8-43bd-99c4-b073c4dd47ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "blob_name = 'model_0519.h5'\n",
    "client = storage.Client()\n",
    "\n",
    "blob = bucket.blob(blob_name)\n",
    "blob.upload_from_filename(blob_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b2ccef-7d27-4f40-a83d-bb39b0692049",
   "metadata": {},
   "source": [
    "Using convolutional layers in a neural network for text data can be beneficial for several reasons:\n",
    "\n",
    "Local Pattern Extraction: Convolutional layers can effectively capture local patterns and features in the text data. By applying filters of different sizes, the convolutional operation can detect patterns at various levels of granularity. This allows the model to learn relevant features such as n-grams, word combinations, or other local patterns that are indicative of the text's meaning or sentiment.\n",
    "\n",
    "Translation Invariance: Convolutional layers exhibit translation invariance, which means they can recognize patterns regardless of their exact position in the input. In the context of text data, this property is useful because the position of a particular word or phrase in a sentence may not always be critical for understanding its meaning. By capturing patterns irrespective of their location, convolutional layers can provide robust representations that are not overly sensitive to word order.\n",
    "\n",
    "Reduced Parameter Count: Convolutional layers can help reduce the number of parameters in the model compared to fully connected layers. This reduction is achieved by weight sharing through the use of filters. By sharing weights, the model can capture the same pattern or feature across different positions in the input, resulting in fewer trainable parameters. This parameter efficiency can make the model easier to train and less prone to overfitting, especially when dealing with limited amounts of text data.\n",
    "\n",
    "Hierarchical Feature Learning: Deep architectures with multiple convolutional layers can learn hierarchical representations of the input text. Lower-level convolutional layers can capture basic local features, while higher-level convolutional layers can learn more complex combinations of these features. This hierarchical learning enables the model to capture both low-level and high-level semantic information from the text.\n",
    "\n",
    "It's worth noting that while convolutional neural networks (CNNs) have primarily been associated with image processing tasks, they have been successfully adapted for natural language processing (NLP) tasks, including text classification. The convolutional operations in text CNNs are typically performed along the time dimension (i.e., word or character sequences) rather than across spatial dimensions (as in image CNNs).\n",
    "\n",
    "That being said, the effectiveness of using convolutional layers in text classification tasks may vary depending on the specific dataset and problem. It's recommended to experiment with different architectures and compare the performance with other approaches, such as recurrent neural networks (RNNs) or transformers, to determine the best choice for your particular task.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Regenerate response"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m108",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m108"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
